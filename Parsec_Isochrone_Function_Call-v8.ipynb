{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56dd268",
   "metadata": {},
   "source": [
    "# Fast isochrone fitting of stellar parameters, in particular ages,   <br>  constrained by spectropic labels, photometry and distance\n",
    "\n",
    "<br>\n",
    "(Version 7, Sep 1, 2023; HWR)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font size=\"3\">\n",
    "This notebook, triggered by Gaia DR3 data in the Galactic disk and in the LMC, aims to determine  stellar parameters via 'isochrone fitting' -- in particular stellar masses and ages -- for large ensembles stars for which we have stellar labels (esp. T$_{eff}$, log$g$, and [M/H]) along with photometry in a number of bands (here, BP, G, RP, J, H and K), and an estimate of the distance. It presumes these quantities and their uncertainties are known. In the current version, it presumes no external knowledge of the extinction (A$_G$), which it also determines.\n",
    "<br>\n",
    "In an effort to make the code fast -- so that $10^6$ objects can be done on a laptop -- the code avoids any isochrone interpolation. The basic logical flow of the code is as follows:\n",
    "    \n",
    "+ ingest an extensive grid of isochrones (here Parsec isochriones), that is presumed to be densely sampled enough in [M/H] and logAge. \"Densely enough\" here means simply being more finely sampled than the final uncertainties in the estimate. It also assumes implicitly that each isochrone -- which is a sequence of points in initial stellar mass M$_{init}$ (e.g. Parsec) or effective evolutionary state, EES (e.g. MIST) -- is finely enough sampled in M$_{init}$. Here, finely enough' is a tricky issue, as it also means the 'observable' (T$_{eff}$ ir photometry) don't change too much. By construction, good isochrone grids come close, in the evolutionary phases that matter.\n",
    "Here, we use M$_{init}$ and Parsec.\n",
    "    \n",
    "+ then create copies of this isochrone grid that are -- in their photometric predictions -- reddened by a (sufficient) set of A$_G$'s. So that we are left with a (yes, large) grid in 4D space: [M/H], logAge, A$_G$ and \n",
    "M$_{init}$. This grid is one part of an Table, with the other part being the \"observational predictions\" for each of these grid points. Here: T$_{eff}$, log$g$, [M/H]) BP, G, RP, J, H and K.\n",
    "    \n",
    "+ the basic \"fitting approach\" for any given data point is sketched here (ADD LINK). It is a proper Bayesian estimate of the stellar parameters' posterior, described by the mean values of [M/H], logAge, A$_G$ and \n",
    "M$_{init}$ and their uncertainties (also the full covariance matrix). The \"trick\" to make this fast is to effect all this by a weighted sum over the pertinent subset of the isochrone points. This brings up two issues:\n",
    "    \n",
    "    * quickly find the subset of (also) reddened isochrone points, where the data likelihood is not vanishing. Here this is done by boolean selection conditions to get only isochrone points within $7\\sigma$ of the observed T$_{eff}$, G and K; more conditions could be added (e.g. $A_G$), which would speed up the code.\n",
    "    \n",
    "    * then, the posterior parameter estimates and their uncertainties are calculated as a weighted some over the pre-selected isochrone points.  There are three kinds of weights that come in:\n",
    "        * the $\\exp\\bigl ( - \\chi^2(\\mathrm{data, uncertainties, model\\  predictions})\\bigr )$\n",
    "        * the isochrone point weight, which accounts for the fact that rapid evolutionary phases are finely sampled in isochroned. Within an isochrone, we sum over isochrone points (instead of over M$_{init}$) we need the Jacobian $d M_{init}/dN_{isochrones}$. This \"penalizes\"  isochrone points that are shortlived.\n",
    "        * while we take priors in [M/H] and logAge to be flat, we account for the (Kroupa-esque) mass function, via $p_{Kroupa}(M_{init}$). \n",
    "    \n",
    "In the version of Sep 3, 2023 this takes about 90 mili-seconds / object on HWRs notebook.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eb659",
   "metadata": {},
   "source": [
    "### settled and open issues  (of this version v7, Sep 1, 2023)\n",
    "\n",
    "\n",
    "* we need to understand which pieces of the data \"dominate\" the fit, the G-band magnitudes?  The path forward here is to look at the difference between the \"data\" and the \"best fit isochrones\". E.g. the metallicities often get shifted around a lot. I.e. there is systematic \"tension\" between the model predictions and the data\n",
    "\n",
    "* do we need to re-weigh spectroscopy and photometry? Currently, I have implemented an 0.03mag floor on the photometric error.\n",
    "\n",
    "* in the current application , a simple distance to the LMC (DM=18.48) is assumed.\n",
    "\n",
    "* do we need to consider a finer grid of extinctions and of (young) ages?\n",
    "\n",
    "* in the end, only data points with spectroscopic labels $3500 K < T_{eff} < 4900 K$ are trustworthy\n",
    "\n",
    "* NOTE: I put some double checking notes in the function call that filters out the right isochrones...\n",
    "\n",
    "* the apparent magnitudes get at some point wholesale converted to absolute magnitudes, with a single distance modulus; this needs attention.\n",
    "\n",
    "\n",
    "### next steps\n",
    "\n",
    "* can we identify members of known clusters for age calibration\n",
    "\n",
    "* identify points that have unusual D$\\chi^2$ weights, and understand why they got selected. This may require getting the a set of chi2s returned, all that are within XXX of the minimum.\n",
    "\n",
    "* understand whether we should create a much finer grid of extinctions, but only select the ones where the observed colors are consistent with Teff and th eexpected reddening range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21608319",
   "metadata": {},
   "source": [
    "set up packages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "\n",
    "from astropy.io import fits as fits\n",
    "\n",
    "from astropy.table import Table,vstack, hstack,Column\n",
    "from astropy.io import ascii\n",
    "from astropy.io.votable import parse_single_table\n",
    "\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import scipy.stats as stats\n",
    "\n",
    "import astropy.table as at\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "from pyia import GaiaData\n",
    "\n",
    "# Set up matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0422e0",
   "metadata": {},
   "source": [
    "#### read in pre-downloaded isochrones; this version uses the Parsec grid from Josh Povick, subsampled in both [M/H] and at (older) ages; but augmented at young ages.\n",
    "\n",
    "#### set the right work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '/Users/rix/Science/Projects/GAIA/GaiaDR3/BP-RP/LMC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14854b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from povick\n",
    "# isochrone = 'parsec36_Povick_sparsed.fits'     # obsolete now\n",
    "isochrone = 'parsec36_Povick_sparsed_young_added.fits'\n",
    "iso = at.Table.read(workdir+isochrone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "### it annoys HWR, so logT --> Teff\n",
    "# ... i.e. switch to linear temperatures\n",
    "iso['logTe'] = 10.** iso['logTe'] \n",
    "iso.rename_column('logTe', 'Teff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The table reflecting the input isochrone grid has',len(iso),' rows at present, with the following coverage: ')\n",
    "plt.scatter(iso['logAge'],iso['MH'],s=1)\n",
    "plt.xlabel('$\\log_{10} ~\\mathrm{age} [yrs]$')\n",
    "plt.ylabel('[M/H]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352d977",
   "metadata": {},
   "source": [
    "### What's in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The isochrone table has the following columns: ',iso.columns)\n",
    "#and how many rows does it have\n",
    "print(' there are a total of ',len(iso),' rows')\n",
    "#print('the data types are',iso[:].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a visual check\n",
    "print(iso[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa5768",
   "metadata": {},
   "source": [
    "###   We now calculate the \"weights\" (or probability) of each isochrone point\n",
    "* we presume that the probability is uniforem with respect to (logAge) and MH\n",
    "* we will implement the IMF probability, presuming a kroupa IMF\n",
    "* the most important part is the $\\frac{dM_{init}}{d{\\mathrm \\ grid\\  point}}$, which is calculated in the step below\n",
    "* in the end we will add $-2\\times ln(p_{grid})$ to the $\\chi^2$ of each grid point, given a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6691a",
   "metadata": {},
   "source": [
    "##### first auxiliary step: define a Kroupa normalized MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c95641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kroupa_imf(m, m_min=0.01, m_max=100, m1=0.08, m2=0.5, alpha1=0.3,alpha2=1.3,alpha3=2.3):\n",
    "    \n",
    "    # Calculate normalization constants for each segment to ensure continuity\n",
    "    A1 = (alpha2 - alpha1) / (m1**(alpha2 - alpha1) - m_min**(alpha2 - alpha1))\n",
    "    A2 = A1 * m1**(alpha2 - alpha1)\n",
    "    A3 = A2 * m2**(alpha3 - alpha2)\n",
    "    \n",
    "    # Calculate the contributions to the normalization factor from each segment\n",
    "    norm1 = A1 / (1 - alpha1) * (m1**(1 - alpha1) - m_min**(1 - alpha1))\n",
    "    norm2 = A2 / (1 - alpha2) * (m2**(1 - alpha2) - m1**(1 - alpha2))\n",
    "    norm3 = A3 / (1 - alpha3) * (m_max**(1 - alpha3) - m2**(1 - alpha3))\n",
    "    \n",
    "    # Calculate the total normalization factor\n",
    "    norm_factor = norm1 + norm2 + norm3\n",
    "    \n",
    "    # Define the IMF\n",
    "    if m < m1:\n",
    "        return A1 * m**(-alpha1) / norm_factor\n",
    "    elif m1 <= m < m2:\n",
    "        return A2 * m**(-alpha2) / norm_factor\n",
    "    else:\n",
    "        return A3 * m**(-alpha3) / norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33803af",
   "metadata": {},
   "source": [
    "#### Now, take add the two components of the weighting, the intrinsic speed of the isochrone evolution at that point, and the IMF probability of having a star at that mass. This will simply become a value-added column in the overall isochrone table.\n",
    "\n",
    "* The intrinsic speed of the isochrone evolution at any point is simply quantified as the $d M_{init}/dN_{isochrones}$, which will end up in a new table column, iso\\['DMini'\\]\n",
    "\n",
    "* We express both the isochrone point weight and the IMF weight via a $\\chi^2$ equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate the probability of mass \"Mini\" given a Kroupa IMF\n",
    "kroupa_probability = [kroupa_imf(m) for m in iso['Mini']]\n",
    "\n",
    "# Create a new column 'DMini' filled with zeros initially\n",
    "iso['DMini'] = 1.e-14\n",
    "\n",
    "# Loop through the rows of the table from the second row to the second-last row\n",
    "for i in range(1, len(iso) - 1):\n",
    "    if iso['Mini'][i + 1] > iso['Mini'][i] and iso['Mini'][i - 1] < iso['Mini'][i]:\n",
    "        iso['DMini'][i] = 0.5 * (iso['Mini'][i + 1] - iso['Mini'][i - 1])\n",
    "    else:\n",
    "        iso['DMini'][i] = 1.e-14\n",
    "        \n",
    "# this is a crucial step. It calculates the prior probability of each isochrone point\n",
    "iso['Dchi2_corr'] = -2*np.log(iso['DMini']) - 2*np.log(kroupa_probability)\n",
    "\n",
    "#print(iso['Dchi2_corr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fd821",
   "metadata": {},
   "source": [
    "### Create dust-reddened versions of the isochrone predictions\n",
    "\n",
    "##### Why?\n",
    "In the end, we want to find the isochrone points that best fit the data, both the spectroscopic parameters and the photometry. For a given set of spectroscopic parameters, the reddening changes the photometry\n",
    "\n",
    "##### Why so early in the code?\n",
    "\n",
    "-- we have to do it only once\n",
    "\n",
    "-- we want to index the final rows of the isochrone table at the start for \n",
    "\n",
    "#### What needs to be done?\n",
    "\n",
    "1) We define the reddening vector RV (Cardelli) and an array of Av values (see in cell).\n",
    "\n",
    "2) We add a zero-Av column to the original table iso.\n",
    "\n",
    "3) We initialize an empty list all_tables to store the original and modified tables.\n",
    "\n",
    "4) A for-loop iterates over each Av value. Inside the loop, we create a modified version of iso_pred by adding Av * RV to the columns 'G_BPmag','Gmag','G_RPmag','Jmag','Hmag','Ksmag'.\n",
    "####!!! the column names are hardwired at the moment !!!\n",
    "\n",
    "5) We add a new 'Av' column to the modified table with the applied Av value.\n",
    "\n",
    "6) Finally, we vertically stack all the tables together to create iso_pred_ext, which contains the original and modified versions of iso_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a590cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are RV valies for BP,G,RP,J,H,K from Cardelli (via Povick)\n",
    "RV = np.array([1.21821835, 1., 0.77005965, 0.34260963, 0.21702019, 0.13758333])\n",
    "# this sets the reddening grid, and needs to be re-considered\n",
    "Av = np.array([0., 0.03, 0.06, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.2, 1.6, 2.1])\n",
    "## temp for the large table\n",
    "#Av = np.array([0., 0.06, 0.1, 0.15, 0.2,  0.3,  0.5, 0.9])\n",
    "\n",
    "# Add a zero-Av column to the original table\n",
    "iso['Av'] = np.zeros(len(iso))\n",
    "\n",
    "# Initialize list to store modified tables\n",
    "all_tables = [iso]\n",
    "\n",
    "# Loop over each Av value to create modified tables\n",
    "for av in Av[1:]:  # Skip the first value (0) as the original table already covers it\n",
    "    mod_table = iso.copy()  # Make a copy of the original table\n",
    "    ### column names hardwired here !!!\n",
    "    for i, col_name in enumerate(['G_BPmag','Gmag','G_RPmag','Jmag','Hmag','Ksmag']):\n",
    "        mod_table[col_name] += av * RV[i]  # Apply the reddening\n",
    "    mod_table['Av'] = np.full(len(mod_table), av)  # Fill the 'Av' column with the current Av value\n",
    "    all_tables.append(mod_table)\n",
    "\n",
    "# Combine all tables\n",
    "iso_Av = vstack(all_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b43e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This A$_G$-augmented isochrone table has the following columns: ',iso_Av.columns)\n",
    "#and how many rows does it have\n",
    "print('  ')\n",
    "print('There are a total of ',len(iso_Av),' rows. Yes, that many!')\n",
    "#print('the data types are',iso[:].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58350012",
   "metadata": {},
   "source": [
    "Let's make a plot to verify that the addition of reddening worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c576b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obsolete\n",
    "#Teff = iso_Av['Teff']\n",
    "#cc = iso_Av['Jmag']-iso_Av['Ksmag']\n",
    "#plt.scatter(Teff,cc,s=1,c=iso_Av['Av'])\n",
    "#plt.ylim(-0.5,5.5)\n",
    "#plt.xlim(3500.,17500.)\n",
    "#plt.colorbar(label='A_V')\n",
    "#plt.gca().invert_xaxis()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df9780",
   "metadata": {},
   "source": [
    "#### Add a column that is the isochrone table's row index. We will need that for book-keeping later\n",
    "\n",
    "(actually, that was for an earlier version... obsolete, I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2570ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_Av['index'] = np.arange(len(iso_Av))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a7a47",
   "metadata": {},
   "source": [
    "### Now split the isochrone table into two parts:\n",
    "\n",
    "* #### the first contains the physical input parameters: metallicity, age and initial mass; and $A_G$\n",
    "\n",
    "* #### the second contains the corresponding predictions for the spectroscopic and photometric \"observables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742de709",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_in = iso_Av['MH', 'logAge','Mini','Av','Dchi2_corr','index']\n",
    "\n",
    "### the full array of observables would look like this\n",
    "#temp = iso['logL','logTe','logg','Gmag','G_BPmag','G_RPmag',\n",
    "#                          'Jmag','Hmag','Ksmag','W1mag','W2mag']\n",
    "# for the time being, we use only\n",
    "iso_pred = iso_Av['Teff','logg','MH','Gmag','G_BPmag','G_RPmag',\n",
    "                          'Jmag','Hmag','Ksmag','index']\n",
    "print(' ')\n",
    "print('There still are a total of ',len(iso_pred),' rows')\n",
    "#print('the data types are',iso[:].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04585166",
   "metadata": {},
   "source": [
    "### necessary function definitions\n",
    "\n",
    "* #### filter_table:\n",
    "    For one object (obs), find the subset of icohrone points whose predictions are 7$\\sigma$ compatible with basic observations.\n",
    "    \n",
    "* #### chi2_for_one_object\n",
    "    This has two parts, first identify the isochrone subset (via filter_table); then return (via compute_chi2) \n",
    "    the array of data likelihoods (as a $\\chi^2$ array). \n",
    "\n",
    "* #### compute_chi2\n",
    "    Just compute the vector of $\\chi^2$'s as  (data - model / uncertainty)$^2$, like you learned in school.\n",
    "    \n",
    "* #### find_indices_within_chi2_limit\n",
    "    Take what \"chi2_for_one_object\" returned for one object, and select an even smaller subset, namely those whose \n",
    "    $\\chi^2$ values lie within $\\Delta\\chi^2$ of the best value. This is used to then calculate the first moment (best value) and variances of the stellar parameters to be estimated (see 'get_means_and_covariances', below). <br> **Note:** by the time this gets called, the isochrone point weights and the IMF weight has already been added (as a chi2-term).\n",
    "    \n",
    "* #### get_means_and_covariances\n",
    "    Taking the points from 'find_indices_within_chi2_limit', it calculates the means and rms of 'MH', 'logAge', 'Mini', 'Av', **by a weighted sum over the preselected gridpoints.** It returns those, along with a covariance matric (This covariance matric is currently not used, as I have not fixed problems making one column of the output table  matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b663701",
   "metadata": {},
   "source": [
    "#### then we define a function call that selects from the whole isochrone table those rows whose Ksmag agrees (withon 0.3mag) with that of the object we are trying to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_table(obs,stddev,iso_pred):\n",
    "    # now set some preconditions to discard all isochrone points that will miss\n",
    "    # basic observables by more than, say, 7 sigma.\n",
    "    start_wall2 = time.time()\n",
    "    nsig = 5\n",
    "    Teff,dTn = obs[0],nsig * stddev[0]\n",
    "    dTeff = dTn if 50 <= dTn <= 200 else min(max(dTn, 50), 200)\n",
    "    MH,dMH = obs[2],nsig * stddev[2]\n",
    "    dMH = dMH if 0.1 <= dMH <= 0.5 else min(max(dMH, 0.1), 0.5)\n",
    "    Gmag,dGn = obs[3],nsig * stddev[3]\n",
    "    dGmag = dGn if 0.04 <= dGn <= 0.3 else min(max(dGn, 0.04), 0.3)\n",
    "    Ksmag,dKn = obs[7],nsig * stddev[7]\n",
    "    dm = dKn if 0.06 <= dKn <= 0.3 else min(max(dKn, 0.06), 0.3)\n",
    "    #print(Teff,stddev[0],dTeff, Gmag, stddev[3],dGmag,Ksmag, stddev[7], dm)\n",
    "    #input(\"Press Enter to continue...\")\n",
    "    # Filter rows where 'Ksmag' < 0\n",
    "    filtered_rows = iso_pred[ \n",
    "                            (iso_pred['Ksmag']<Ksmag+dm) * (iso_pred['Ksmag']>Ksmag-dm)*\n",
    "                            (iso_pred['Teff']<Teff+dTeff) * (iso_pred['Teff']>Teff-dTeff)*\n",
    "                            (iso_pred['MH']<MH+dMH) * (iso_pred['MH']>MH-dMH)*\n",
    "                            (iso_pred['Gmag']<Gmag+dGmag) * (iso_pred['Gmag']>Gmag-dGmag)\n",
    "                            ]\n",
    "    \n",
    "    if len(filtered_rows) > 10:\n",
    "        #print(len(filtered_rows),' which took', 1000.*(time.time()-start_wall2),' milliseconds')\n",
    "        # Create a new table with all columns except 'index'\n",
    "        return filtered_rows[[col for col in iso_pred.colnames if col != 'index']],filtered_rows['index']\n",
    "    else:\n",
    "        #print('found no nearby isochrone points; retaining full isochrone')\n",
    "        return iso_pred[[col for col in iso_pred.colnames if col != 'index']],iso_pred['index']\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_for_one_object(obs,stddev,isochrones):\n",
    "    # we start by pearing out the (snmall) subset of isochrones that has the right Ksmag, which obs[7]\n",
    "    subset,indices = filter_table(obs,stddev,isochrones)\n",
    "    # now turn the subset into an 2D array, and check that its dimensions are compatible with the observations\n",
    "    subset_array = np.array([subset[col] for col in subset.columns]).T\n",
    "    #print(subset_array.shape,obs.shape,variance.shape)\n",
    "    return compute_chi2(obs, stddev**2, subset_array),indices\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522a7af",
   "metadata": {},
   "source": [
    "this call draws on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chi2(data, variances, model):\n",
    "    # Make sure dimensions match\n",
    "    if data.shape[0] != variances.shape[0]:\n",
    "        raise ValueError(\"Data and variances must have the same length\")\n",
    "        \n",
    "    if model.shape[1] != data.shape[0]:\n",
    "        raise ValueError(\"Number of columns in model must match length of data\")\n",
    "        \n",
    "    # Reshape data and variances to enable broadcasting in NumPy\n",
    "    data = data.reshape(1, -1)\n",
    "    variances = variances.reshape(1, -1)\n",
    "    \n",
    "    # Compute the chi2 values\n",
    "    diff = data - model  # shape will be (K, N)\n",
    "    chi2_values = np.sum((diff ** 2) / variances, axis=1)\n",
    "    chi2_values[chi2_values <= 0] = 1.e-6\n",
    "    \n",
    "    return chi2_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07eb19",
   "metadata": {},
   "source": [
    "#### and one more set of function calls, whose purpose it is to take -- for a given object -- als the isochrone points with a chi2 not too much worse (Dchi2_lim) than the chi2 minimum, and calculate age, M/H etc.. statistic from it.\n",
    "\n",
    "* the first one  returns the indices of all the elements in the chi2 array\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices_within_chi2_limit(chi2s, Dchi2_lim):\n",
    "    min_value = chi2s[np.argmin(chi2s)]\n",
    "    within_limit_indices = np.where(np.abs(chi2s - min_value) <= Dchi2_lim)[0]\n",
    "    return within_limit_indices\n",
    "\n",
    "def get_means_and_covariances(chi2s, iso_in):\n",
    "    # Calculate the weights based on chi2s\n",
    "    weights = np.exp(-chi2s)\n",
    "    \n",
    "    # Normalize the weights\n",
    "    sum_weights = np.sum(weights)\n",
    "    normalized_weights = weights / sum_weights\n",
    "    \n",
    "    # List of columns\n",
    "    columns = ['MH', 'logAge', 'Mini', 'Av']\n",
    "    \n",
    "    # Initialize lists to hold means\n",
    "    means = []\n",
    "    \n",
    "    # Loop through each column to calculate weighted mean\n",
    "    for column in columns:\n",
    "        # Calculate the weighted mean\n",
    "        weighted_mean = np.sum(iso_in[column] * normalized_weights)\n",
    "        means.append(weighted_mean)\n",
    "        \n",
    "    # Initialize covariance matrix\n",
    "    covariance_matrix = np.zeros((4, 4))\n",
    "    \n",
    "    # Loop through each pair of columns to calculate the covariance\n",
    "    for i, col1 in enumerate(columns):\n",
    "        for j, col2 in enumerate(columns):\n",
    "            deviations1 = iso_in[col1] - means[i]\n",
    "            deviations2 = iso_in[col2] - means[j]\n",
    "            \n",
    "            covariance_matrix[i, j] = np.sum(normalized_weights * deviations1 * deviations2)\n",
    "            \n",
    "    # Convert list to numpy array for returning\n",
    "    means = np.array(means)\n",
    "    \n",
    "    return means, covariance_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90811e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Now comes the grand chi2 call\n",
    "This takes a 1D array \"data\", where entries constitute N attributes of a given object. The function also accepts a 1D array \"variances\" that has the same shape, but where the entries reflects the variance in the estimate  of each \"data\" attribute. In addition, this function also accepts a 2D array \"model\" that has shape K x N, i.e. K rows and the same number of columns as the dimension of \"data\". For each row k=1,K the function than calculates the chi2 of Sum_1^N (data-model)^2/variances^2, and returns a 1D array of chi2 that has length K.\n",
    "\n",
    "Here the \"data\" are the observables of one object, from Teff to Ksmag; here of length 7. The \"variances\" are their uncertainties; and K are the possible isochrone prodictions, each of length 7.\n",
    "\n",
    "There is some scary index book-keeping, such as <br>\n",
    "interim_table = hstack(\\[ids[:i],obs_table[:i],fit_results,iso_in[indx_best[:i]],iso_pred[indx_best[:i]]\\])\n",
    "This is where chapt GPT4 pro's vectorization advice becomes scary; seems to work, though.\n",
    "\n",
    "\n",
    "The parameter 'nupdate' sets after how many objects you get an on-screen update. And, every tiime also an interim file with the 'results so far' gets dumped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stellar_params(obs_table,std_table,ids,iso_pred,iso_in,nupdate):\n",
    "    Dchi2_lim = 7.    # this is the chi2 limit (among isochgrone points)\n",
    "                      #around the best value where we calculate statistics\n",
    "    # convert table to numpy arrays here\n",
    "    obs = np.vstack([obs_table[col].data for col in obs_table.columns])\n",
    "    std = np.vstack([std_table[col].data for col in std_table.columns])\n",
    "    \n",
    "    # now set up the structure for what he fit returns\n",
    "    indx_best = []\n",
    "    chi2_best = []\n",
    "    fit_results = Table(\n",
    "    names=('fitMH', 'fitlogAge', 'fitMini', 'fitAv', 'varMH', 'varlogAge', 'varMini', 'varAv', 'covariances'),\n",
    "    dtype=('f8', 'f8', 'f8', 'f8', 'f8', 'f8', 'f8', 'f8', 'O'))\n",
    "\n",
    "    start_wall = time.time()\n",
    "    for i in range(obs.shape[1]):\n",
    "        chi2s,indx = chi2_for_one_object(obs[:,i],std[:,i],iso_pred)\n",
    "                \n",
    "        # now do the \"gridpoint probability\" correction to chi2\n",
    "        chi2s = chi2s + iso_in['Dchi2_corr'][indx]\n",
    "\n",
    "        # now select all the isochrone points with \"pretty good\" chi2\n",
    "        indx_good = find_indices_within_chi2_limit(chi2s, Dchi2_lim)\n",
    "        #print(chi2s[indx_good],iso_pred['Gmag'][indx[indx_good]])\n",
    "        #input(\"Press Enter to continue...\")\n",
    "        \n",
    "        #plt.scatter(iso_pred['Teff'][indx[indx_good]],iso_pred['Gmag'][indx[indx_good]])\n",
    "        #plt.show()\n",
    "        means,covariance_matrix = get_means_and_covariances(chi2s[indx_good],iso_in[indx[indx_good]])\n",
    "        uncertainties = np.sqrt(np.diag(covariance_matrix))\n",
    "    \n",
    "\n",
    "        # Append a new row to the fit_results table\n",
    "        fit_results.add_row([means[0], means[1], means[2], means[3],\n",
    "                     uncertainties[0], uncertainties[1], uncertainties[2], uncertainties[3],\n",
    "                     covariance_matrix])\n",
    "        # check...\n",
    "        #print(fit_results[-1])\n",
    "        #input(\"Press Enter to continue...\")\n",
    "        \n",
    "        #mean_values,covariance_matrix = get_means_and_covariances(chi2s[indx_good],iso_in[indx[indx_good]])\n",
    "        #uncertainties = np.sqrt(np.diag(covariance_matrix))\n",
    "        #print(mean_values)\n",
    "        #C\n",
    "\n",
    "         \n",
    "        #ibest = indx[np.argmin(chi2s)]\n",
    "        chi2_best.append(np.argmin(chi2s))\n",
    "        indx_best.append(indx[np.argmin(chi2s)])\n",
    "        \n",
    "        \n",
    "        #print(iso_in[indx[np.argmin(chi2s)]])\n",
    "        #print(np.min(chi2s))\n",
    "        if (i % nupdate)== 0:\n",
    "            if (i >0):\n",
    "                print('Have done',i,'stars so far. This took',time.time()-start_wall,' seconds')\n",
    "                #print('mean values for this objects are:',means,uncertainties) #,iso_pred[indx[np.argmin(chi2s)]])\n",
    "                \n",
    "                interim_table = hstack([ids[:i],obs_table[:i],fit_results,iso_in[indx_best[:i]],iso_pred[indx_best[:i]]])\n",
    "                columns_to_keep = [name for name in interim_table.colnames if name != 'covariances']\n",
    "                new_table = interim_table[columns_to_keep]\n",
    "                new_table.write(workdir+'interim_table.fits', format='fits',overwrite=True)\n",
    "                #input(\"Press Enter to continue...\")\n",
    "    end_wall = time.time()\n",
    "    print('The age/mass estimate for these',obs.shape[1],' objects took ',\n",
    "          int((end_wall - start_wall)),' seconds ')\n",
    "    \n",
    "    \n",
    "    # now put together a table to return\n",
    "    iso_in_sub = iso_in[indx_best]\n",
    "    iso_pred_sub = iso_pred[indx_best]\n",
    "    if 'index' in iso_pred_sub.colnames:\n",
    "        iso_pred_sub.remove_column('index')\n",
    "        iso_pred_sub.remove_column('MH')\n",
    "        \n",
    "    # make sure the best-fit chi2 can be added\n",
    "    chi2obs_column = Column(data=chi2_best, name='chi2obs')\n",
    "    obs_table.add_column(chi2obs_column)\n",
    "    #print('iso_pred check',iso_pred_sub.colnames)\n",
    "    # make sure that all tables have the same number of rows\n",
    "    assert len(iso_in_sub) == len(iso_pred_sub) == len(obs_table) == len(ids) == len(chi2_best) == len(fit_results)\n",
    "    # Horizontally stack the tables\n",
    "    combined_table = hstack([ids,obs_table,fit_results,iso_in_sub,iso_pred_sub])\n",
    "    #return iso_in[indx_best]\n",
    "    return combined_table\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35713b9d",
   "metadata": {},
   "source": [
    "## Now we try  out things\n",
    "\n",
    "### We'll load in Josh's test data from the LMC\n",
    "\n",
    "... and convert the to \"absolute magnitude\" using the star-by-star distances from a flit inclined model (REF)\n",
    "\n",
    "... and separate the \"observables\" from the \"standard deviations\" and dump all this into numpy arrays (probably shouldn't have)\n",
    "\n",
    ".... note, rows with NaNs get eliminated; this may need attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bea84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the right data table: just RGB stars, or all?\n",
    "#lmc = Table(fits.getdata(workdir+'lmc_data_rgb.fits')) \n",
    "lmc = Table(fits.getdata(workdir+'lmc_data_allXPStars.fits')) \n",
    "\n",
    "#N = 900  # Replace with the actual number of rows you want\n",
    "#lmc = lmc[:N]\n",
    "\n",
    "#Remove rows containing NaNs\n",
    "mask = np.all(~np.isnan(lmc.to_pandas().values), axis=1)\n",
    "lmc = lmc[mask]\n",
    "\n",
    "print(lmc.columns) \n",
    "\n",
    "# columns to be converted to \"absolute magnitude\"\n",
    "cols_to_modify = ['BP', 'G', 'RP', 'J', 'H', 'K']\n",
    "# using the distance modulus of \n",
    "DM = 5*np.log10(lmc['DISTANCE']/10.)\n",
    "\n",
    "# Modify the specified columns\n",
    "for col in cols_to_modify:\n",
    "    lmc[col] = lmc[col] - DM\n",
    "    \n",
    "print('Well, thats  ', len(lmc), ' sources!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f15a4",
   "metadata": {},
   "source": [
    "#### setting a floor to the photometric uncertainties\n",
    "* and also set a floor to logg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set floors that can be different for the different bands; the order is\n",
    "#.    'Jmag', 'Hmag', 'Ksmag', 'Gmag', 'G_BPmag', 'G_RPmag', 'logg'\n",
    "dmin_array = np.array([0.03, 0.03, 0.03 , 0.03, 0.03, 0.03, 0.1])\n",
    "# the next line would be 'spectroscopy only'\n",
    "#dmin_array = np.array([0.6, 0.6, 0.1 , 0.4, 0.4, 0.4, 0.2])\n",
    "\n",
    "# List of column names to modify\n",
    "columns_to_modify = ['J_ERR', 'H_ERR', 'K_ERR', 'G_ERR', 'BP_ERR', 'RP_ERR','LOGG']\n",
    "\n",
    "# Iterate through each column and its corresponding dmin value\n",
    "for col, dmin_value in zip(columns_to_modify, dmin_array):\n",
    "    lmc[col][lmc[col] < dmin_value] = dmin_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0561574",
   "metadata": {},
   "source": [
    "#### now split the table into the \"observables\" and their \"uncertainties\"; also keep track of the source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb059cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmc_ids = lmc['source_id']\n",
    "lmc_obs = lmc['TEFF', 'LOGG','FE_H','G','BP','RP','J','H','K']\n",
    "lmc_std = lmc['TEFF_ERR', 'LOGG_ERR','FE_H_ERR','G_ERR','BP_ERR','RP_ERR','J_ERR','H_ERR','K_ERR']\n",
    "## do we take the original temperatures??\n",
    "#lmc_obs = lmc['TEFF_XP', 'LOGG','FE_H','G','BP','RP','J','H','K']\n",
    "#lmc_std = lmc['TEFF_XP_ERR', 'LOGG_ERR','FE_H_ERR','G_ERR','BP_ERR','RP_ERR','J_ERR','H_ERR','K_ERR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab2295",
   "metadata": {},
   "source": [
    "The cell below, allows for a random subset of objects to be looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(lmc_ids)\n",
    "\n",
    "# Generate 1000 random indices\n",
    "random_indices = np.random.choice(num_rows, 100000, replace=False)  # replace=False ensures unique indices\n",
    "\n",
    "# Create a new table containing only the randomly chosen rows\n",
    "lmc_rids = lmc_ids[random_indices]\n",
    "lmc_robs = lmc_obs[random_indices]\n",
    "lmc_rstd = lmc_std[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc231a",
   "metadata": {},
   "source": [
    "## The next Cell is where it all happens\n",
    "\n",
    "We feed it data and isochrones, and it return as suitably formatted Table (here dubbed age_fits), that contains input data and output fit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53852de",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_fits = stellar_params(lmc_obs,lmc_std,lmc_ids,iso_pred,iso_in,10000)\n",
    "#age_fits = stellar_params(lmc_robs,lmc_rstd,lmc_rids,iso_pred,iso_in,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f859a0",
   "metadata": {},
   "source": [
    "This is an interim cell, that throws out the \"covariance matrix' column, before writing to a fits file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [name for name in age_fits.colnames if name != 'covariances']\n",
    "new_table = age_fits[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac13c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table.write(workdir+'age_fitting.Sep3.2023.v1.fits', format='fits',overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
